import torch.nn as nn
import torch.nn.functional as F
import torch
import math


class FPN(nn.Module):
    '''only for resnet50,101,152'''

    def __init__(self, features=256, use_p5=True):
        super(FPN, self).__init__()
        self.prj_5 = nn.Conv2d(2048, features, kernel_size=1)
        self.prj_4 = nn.Conv2d(1024, features, kernel_size=1)
        self.prj_3 = nn.Conv2d(512, features, kernel_size=1)
        self.conv_5 = nn.Conv2d(features, features, kernel_size=3, padding=1)
        self.conv_4 = nn.Conv2d(features, features, kernel_size=3, padding=1)
        self.conv_3 = nn.Conv2d(features, features, kernel_size=3, padding=1)
        if use_p5:
            self.conv_out6 = nn.Conv2d(features, features, kernel_size=3, padding=1, stride=2)
        else:
            self.conv_out6 = nn.Conv2d(2048, features, kernel_size=3, padding=1, stride=2)
        self.conv_out7 = nn.Conv2d(features, features, kernel_size=3, padding=1, stride=2)
        self.use_p5 = use_p5
        self.apply(self.init_conv_kaiming)

    def upsamplelike(self, inputs):
        src, target = inputs
        return F.interpolate(src, size=(target.shape[2], target.shape[3]),
                             mode='nearest')

    def init_conv_kaiming(self, module):
        if isinstance(module, nn.Conv2d):
            nn.init.kaiming_uniform_(module.weight, a=1)

            if module.bias is not None:
                nn.init.constant_(module.bias, 0)

    def forward(self, x):
        C3, C4, C5 = x
        P5 = self.prj_5(C5)
        P4 = self.prj_4(C4)
        P3 = self.prj_3(C3)

        P4 = P4 + self.upsamplelike([P5, C4])
        P3 = P3 + self.upsamplelike([P4, C3])

        P3 = self.conv_3(P3)
        P4 = self.conv_4(P4)
        P5 = self.conv_5(P5)

        P5 = P5 if self.use_p5 else C5
        P6 = self.conv_out6(P5)
        P7 = self.conv_out7(F.relu(P6))
        return [P3, P4, P5, P6, P7]


class FPN_multispectral(nn.Module):
    '''only for resnet50,101,152'''

    def __init__(self, features=256, use_p5=True):
        super(FPN_multispectral, self).__init__()
        # rgb fpn
        self.prj_5_r = nn.Conv2d(2048, features, kernel_size=1)  # conv 1*1 C5->P5 VIS
        self.prj_4_r = nn.Conv2d(1024, features, kernel_size=1)  # conv 1*1 C4->P4 VIS
        self.prj_3_r = nn.Conv2d(512, features, kernel_size=1)  # conv 1*1 C3->P3 VIS
        self.conv_5_r = nn.Conv2d(features, features, kernel_size=3, padding=1)  # conv 3*3 P5 output VIS
        self.conv_4_r = nn.Conv2d(features, features, kernel_size=3, padding=1)  # conv 3*3 P4 output VIS
        self.conv_3_r = nn.Conv2d(features, features, kernel_size=3, padding=1)  # conv 3*3 P3 output VIS
        # ir fpn
        self.prj_5_i = nn.Conv2d(2048, features, kernel_size=1)  # conv 1*1 C5->P5 IR
        self.prj_4_i = nn.Conv2d(1024, features, kernel_size=1)  # conv 1*1 C4->P4 IR
        self.prj_3_i = nn.Conv2d(512, features, kernel_size=1)  # conv 1*1 C3->P3 IR
        self.conv_5_i = nn.Conv2d(features, features, kernel_size=3, padding=1)  # conv 3*3 P5 output IR
        self.conv_4_i = nn.Conv2d(features, features, kernel_size=3, padding=1)  # conv 3*3 P4 output IR
        self.conv_3_i = nn.Conv2d(features, features, kernel_size=3, padding=1)  # conv 3*3 P3 output IR

        if use_p5:  # if use_p5 == True, P6 is generated by P5; Otherwise, P6 is generated by C5
            self.conv_out6_r = nn.Conv2d(features, features, kernel_size=3, padding=1,
                                         stride=2)  # conv 1*1 P6 output VIS
            self.conv_out6_i = nn.Conv2d(features, features, kernel_size=3, padding=1,
                                         stride=2)  # conv 1*1 P6 output IR
        else:
            self.conv_out6_r = nn.Conv2d(2048, features, kernel_size=3, padding=1, stride=2)  # conv 1*1 P6 output VIS
            self.conv_out6_i = nn.Conv2d(2048, features, kernel_size=3, padding=1, stride=2)  # conv 1*1 P6 output IR

        self.conv_out7_r = nn.Conv2d(features, features, kernel_size=3, padding=1, stride=2)  # conv 1*1 P7 output VIS
        self.conv_out7_i = nn.Conv2d(features, features, kernel_size=3, padding=1, stride=2)  # conv 1*1 P7 output IR

        self.DMAF_3 = DMAF()  # DMAF P3
        self.DMAF_4 = DMAF()  # DMAF P4
        self.DMAF_5 = DMAF()  # DMAF P5
        self.DMAF_6 = DMAF()  # DMAF P6
        self.DMAF_7 = DMAF()  # DMAF P7

        self.use_p5 = use_p5  # if use_p5 == True, P6 is generated by P5; Otherwise, P6 is generated by C5
        self.apply(self.init_conv_kaiming)  # weight initialization

    def upsamplelike(self, inputs):
        src, target = inputs
        return F.interpolate(src, size=(target.shape[2], target.shape[3]),  # upsample, linear interpolation
                             mode='nearest')

    def init_conv_kaiming(self, module):
        if isinstance(module, nn.Conv2d):
            nn.init.kaiming_uniform_(module.weight, a=1)

            if module.bias is not None:
                nn.init.constant_(module.bias, 0)

    def forward(self, x_rgb, x_ir):
        C3_r, C4_r, C5_r = x_rgb  # C3, C4, C5, VIS
        C3_i, C4_i, C5_i = x_ir  # C3, C4, C5, IR

        # rgb C->P
        P5_r = self.prj_5_r(C5_r)  # C5->P5, VIS
        P4_r = self.prj_4_r(C4_r)  # C4->P4, VIS
        P3_r = self.prj_3_r(C3_r)  # C3->P3, VIS

        # ir C->P
        P5_i = self.prj_5_i(C5_i)  # C5->P5, IR
        P4_i = self.prj_4_i(C4_i)  # C4->P4, IR
        P3_i = self.prj_3_i(C3_i)  # C3->P3, IR

        P5_r_mix, P5_i_mix = self.DMAF_5(P5_r, P5_i)  # Input: P5_VIS, P5_IR Output: P5_VIS_mix, P5_IR_mix

        P4_r = P4_r + self.upsamplelike([P5_r_mix, C4_r])  # 2*upsample (P5_VIS_mix) + P4_VIS
        P4_i = P4_i + self.upsamplelike([P5_i_mix, C4_i])  # 2*upsample (P5_IR_mix) + P4_IR

        P4_r_mix, P4_i_mix = self.DMAF_4(P4_r, P4_i)  # Input: P4_VIS, P4_IR Output: P4_VIS_mix, P4_IR_mix

        P3_r = P3_r + self.upsamplelike([P4_r_mix, C3_r])  # 2*upsample (P4_VIS_mix) + P3_VIS
        P3_i = P3_i + self.upsamplelike([P4_i_mix, C3_i])  # 2*upsample (P4_IR_mix) + P3_IR

        # P4 = P4 + self.upsamplelike([P5, C4])
        # P3 = P3 + self.upsamplelike([P4, C3])
        P3_r_mix, P3_i_mix = self.DMAF_3(P3_r, P3_i)  # Input: P3_VIS, P3_IR Output: P3_VIS_mix, P3_IR_mix

        P3_r_mix = self.conv_3_r(P3_r_mix)  # P3_VIS_mix -> conv 3*3 --> P3_VIS_output
        P3_i_mix = self.conv_3_i(P3_i_mix)  # P3_IR_mix -> conv 3*3 --> P3_IR_output
        P4_r_mix = self.conv_4_r(P4_r_mix)  # P4_VIS_mix -> conv 3*3 --> P4_VIS_output
        P4_i_mix = self.conv_4_i(P4_i_mix)  # P4_IR_mix -> conv 3*3 --> P4_IR_output
        P5_r_mix = self.conv_5_r(P5_r_mix)  # P5_VIS_mix -> conv 3*3 --> P5_VIS_output
        P5_i_mix = self.conv_5_i(P5_i_mix)  # P5_IR_mix -> conv 3*3 --> P5_IR_output

        # P5_r_mix = P5 if self.use_p5 else C5
        P6_r = self.conv_out6_r(P5_r_mix)  # 2*downsample (P5_VIS_mix)
        P6_i = self.conv_out6_i(P5_i_mix)  # 2*downsample (P5_IR_mix)
        P6_r_mix, P6_i_mix = self.DMAF_6(P6_r, P6_i)  # Input: P6_VIS, P6_IR Output: P6_VIS_mix, P6_IR_mix

        P7_r = self.conv_out7_r(F.relu(P6_r_mix))  # 2*downsample (P6_VIS_mix)
        P7_i = self.conv_out7_i(F.relu(P6_i_mix))  # 2*downsample (P6_VIS_mix)

        P7_r_mix, P7_i_mix = self.DMAF_7(P7_r, P7_i)  # Input: P7_VIS, P7_IR Output: P7_VIS_mix, P7_IR_mix

        return [P3_r_mix, P4_r_mix, P5_r_mix, P6_r_mix, P7_r_mix], [P3_i_mix, P4_i_mix, P5_i_mix, P6_i_mix, P7_i_mix]


class Lambda(nn.Module):
    def __init__(self, func):
        super().__init__()
        self.func = func

    def forward(self, x):
        return self.func(x)


class DMAF(nn.Module):
    def __init__(self):
        super().__init__()
        self.subtracted_r = Lambda(lambda x: torch.subtract(x[0], x[1]))  # rgb subtract ir
        self.subtracted_i = Lambda(lambda x: torch.subtract(x[0], x[1]))  # ir subtract rgb
        self.r_subtracted_weight = nn.AdaptiveAvgPool2d(1)  # global average pooling
        self.i_subtracted_weight = nn.AdaptiveAvgPool2d(1)  # global average pooling
        self.r_excitation_weight = nn.Tanh()  # activation function
        self.i_excitation_weight = nn.Tanh()  # activation function

    def forward(self, x_rgb, x_ir):
        subtracted_r = self.subtracted_r([x_rgb, x_ir])  # rgb subtract ir
        subtracted_weight_r = self.r_subtracted_weight(subtracted_r)  # global average pooling (rgb-ir)
        excitation_weight_r = self.r_excitation_weight(subtracted_weight_r)  # activate (global average pooling (rgb-ir))

        subtracted_i = self.subtracted_i([x_ir, x_rgb])  # ir subtract rgb
        subtracted_weight_i = self.i_subtracted_weight(subtracted_i)   # global average pooling (ir-rgb)
        excitation_weight_i = self.i_excitation_weight(subtracted_weight_i)  # global average pooling (ir-rgb)

        x_r_weight = torch.mul(x_rgb, excitation_weight_r)  # x_rgb * excitation_weight_r
        x_i_weight = torch.mul(x_ir, excitation_weight_i)  # x_ir * excitation_weight_i
        x_r_mix = torch.add(x_i_weight, x_rgb)   # x_i_weight + x_rgb
        x_i_mix = torch.add(x_ir, x_r_weight)    # x_ir + x_r_weight

        return x_r_mix, x_i_mix